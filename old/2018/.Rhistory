# Single run, try this repeatedly
nn <- nnet(y=y.1, x=x.1, linout=TRUE, size=1, maxit=500)
(MSE <- nn$value/nrow(x.1))
(MSPE <- mean((y.2 - predict(nn, x.2))^2))
nn <- nnet(y=y.1, x=x.1, linout=TRUE, size=1, maxit=500)
(MSE <- nn$value/nrow(x.1))
(MSPE <- mean((y.2 - predict(nn, x.2))^2))
nn <- nnet(y=y.1, x=x.1, linout=TRUE, size=1, maxit=500)
(MSE <- nn$value/nrow(x.1))
(MSPE <- mean((y.2 - predict(nn, x.2))^2))
nn <- nnet(y=y.1, x=x.1, linout=TRUE, size=1, maxit=500)
(MSE <- nn$value/nrow(x.1))
(MSPE <- mean((y.2 - predict(nn, x.2))^2))
nn <- nnet(y=y.1, x=x.1, linout=TRUE, size=1, maxit=500)
(MSE <- nn$value/nrow(x.1))
(MSPE <- mean((y.2 - predict(nn, x.2))^2))
nn <- nnet(y=y.1, x=x.1, linout=TRUE, size=1, maxit=500)
(MSE <- nn$value/nrow(x.1))
(MSPE <- mean((y.2 - predict(nn, x.2))^2))
nn <- nnet(y=y.1, x=x.1, linout=TRUE, size=1, maxit=500)
(MSE <- nn$value/nrow(x.1))
(MSPE <- mean((y.2 - predict(nn, x.2))^2))
nn <- nnet(y=y.1, x=x.1, linout=TRUE, size=1, maxit=500)
(MSE <- nn$value/nrow(x.1))
(MSPE <- mean((y.2 - predict(nn, x.2))^2))
#  Show weights: "b" means intercept, "i" is "input variable"
#   "h" is hidden node, and "o" is "output variable"
summary(nn)
plot(x=y.2, y=predict(nn, x.2), main="1 Node, 0 decay")
abline(a=0,b=1)
rm(list=ls())                         # Empty the workspace        #
library(nnet)
aq <- na.omit(airquality[,c("Ozone","Wind","Temp")])
set.seed(298192001)
# Split on a fixed sample size.  Different
train <- sample(x=1:nrow(aq), size = .7*nrow(aq), replace = FALSE)
x.1.unscaled <- aq[train, 2:3]
y.1 <- aq[train,1]
x.2.unscaled <- aq[-train, 2:3]
y.2 <- aq[-train,1]
rescale <- function(x1,x2){
for(col in 1:ncol(x1)){
a <- min(x2[,col])
b <- max(x2[,col])
x1[,col] <- (x1[,col]-a)/(b-a)
}
x1
}
x.1 <- rescale(x.1.unscaled, x.1.unscaled)
#Prove that it worked
apply(X=x.1, MARGIN=2, FUN=min)
apply(X=x.1, MARGIN=2, FUN=max)
x.2 <- rescale(x.2.unscaled, x.1.unscaled)
#Prove that it worked, but does not perfectly scale test set
apply(X=x.2, MARGIN=2, FUN=min)
apply(X=x.2, MARGIN=2, FUN=max)
# Single run, try this repeatedly
nn <- nnet(y=y.1, x=x.1, linout=TRUE, size=1, maxit=500)
(MSE <- nn$value/nrow(x.1))
(MSPE <- mean((y.2 - predict(nn, x.2))^2))
#  Show weights: "b" means intercept, "i" is "input variable"
#   "h" is hidden node, and "o" is "output variable"
summary(nn)
plot(x=y.2, y=predict(nn, x.2), main="1 Node, 0 decay")
abline(a=0,b=1)
# Now try tuning.
# I have created two functions for this:
#   run.nn() takes the input settings for size and decay, runs the
#     NN 100 times, and pulls out the one with the smallest MSE
#   process.nn() takes a nnet object, computes MSE and MSPE, and plots
#     the predictions and observed values for the test set.
?nnet
nn.final
run.nn <- function(siz, dec){
MSE.final <- 9e99
#  check <- MSE.final
for(i in 1:100){
nn <- nnet(y=y.1, x=x.1, linout=TRUE, size=siz, decay=dec, maxit=500, trace=FALSE)
MSE <- nn$value/nrow(x.1)
if(MSE < MSE.final){
MSE.final <- MSE
nn.final <- nn
}
#    check <- c(check,MSE.final)
}
#  check
nn.final
}
process.nn <- function(nnobj){
p.nn <- predict(nnobj, x.1)
MSPE <- mean((y.2 - predict(nnobj,x.2))^2)
MSE <- nnobj$value/nrow(x.1)
plot(x=y.1, y=p.nn,
main=paste(nnobj$n[2],"node, ",nnobj$decay,"decay, MSE=",round(MSE), "MSPE=",round(MSPE)))
abline(a=0,b=1)
}
nn.1_0 <- run.nn(1,0)
nn.1_0
process.nn(nn.1_0)
process.nn(nn.1_0)
process.nn(nn.1_0)
nnobj
nn.1_0
run.nn
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
par(mfrow=c(3,4))
win.graph(h=12,w=16, pointsize=9)
par(mfrow=c(3,4))
### 1 node
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
process.nn <- function(nnobj){
p.nn <- predict(nnobj, x.1)
MSPE <- mean((y.2 - predict(nnobj,x.2))^2)
MSE <- nnobj$value/nrow(x.1)
plot(x=y.1, y=p.nn,
main=paste(nnobj$n[2],"node, ",nnobj$decay,"decay, MSE=",round(MSE), "MSPE=",round(MSPE)))
abline(a=0,b=1)
}
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
# Now try this for 3 different sizes and 4 different decays,
#   and put plots all together
win.graph(h=12,w=16, pointsize=9)
par(mfrow=c(3,4))
# Run 50 times and select model with best *training error*
### 1 node
nn.1_0 <- run.nn(1,0)
process.nn(nn.1_0)
# Add small shrinkage
### 1 node
nn.1_.001 <- run.nn(1,0.001)
process.nn(nn.1_.001)
# Add larger shrinkage
### 1 node
nn.1_.1 <- run.nn(1,0.1)
process.nn(nn.1_.1)
# Add large shrinkage
### 1 node
nn.1_1 <- run.nn(1,1)
process.nn(nn.1_1)
### 2 node
nn.2_0 <- run.nn(2,0)
process.nn(nn.2_0)
# Add small shrinkage
### 2 node
nn.2_.001 <- run.nn(2,0.001)
process.nn(nn.2_.001)
# Add larger shrinkage
### 2 node
nn.2_.1 <- run.nn(2,0.1)
process.nn(nn.2_.1)
# Add large shrinkage
### 2 node
nn.2_1 <- run.nn(2,1)
process.nn(nn.2_1)
### 4 node
nn.4_0 <- run.nn(4,0)
process.nn(nn.4_0)
# Add small shrinkage
### 4 node
nn.4_.001 <- run.nn(4,0.001)
process.nn(nn.4_.001)
# Add larger shrinkage
### 4 node
nn.4_.1 <- run.nn(4,0.1)
process.nn(nn.4_.1)
# Add large shrinkage
### 4 node
nn.4_1 <- run.nn(4,1)
process.nn(nn.4_1)
set.seed(41891019)
reps=10 # Boot Reps, Feel free to increase this to tolerable value
nets=20 # Number of nnets per setting.  May or may not be enough!
siz <- c(1,2,4,6)
dec <- c(0,0.001,0.1,1)
# Set up matrices to hold results. First two columns are parameter values.
#  Each column after that is a rep.
MSPR <- matrix(NA, nrow=length(siz)*length(dec), ncol=reps+2)
MSE <- matrix(NA, nrow=length(siz)*length(dec), ncol=reps+2)
r=1
# Start data resampling loop
for(r in 1:reps){
resamp <- sample.int(n=nrow(aq), size=nrow(aq), replace=TRUE)
x.b1 <- aq[resamp,2:3]
xr <- rescale(x.b1,x.b1)
yr <- aq[resamp,1]
x.b2 <- aq[-unique(resamp),2:3]
xp <- rescale(x.b2, x.b1)
yp <- aq[-unique(resamp),1]
# Set counter for storage of results
qq <- 1
# Cycle over all parameter values
for(s in siz){
for(d in dec){
MSPR[qq,1:2] <- c(s,d)
MSE[qq,1:2] <- c(s,d)
# Run nnet and get MSPE and MSE from run
MSEmin <- 9e99
for(i in 1:nets){
nn <- nnet(y=yr, x=xr, linout=TRUE, size=s, decay=d, maxit=500, trace=FALSE)
MS <- nn$value/nrow(xr)
if(MS < MSEmin){
MSEmin <- MS
p.min <-predict(nn, newdata=xp)
}
}
# Save results in new column of matrix
MSPR[qq, r+2] <- mean((yp - p.min)^2)
MSE[qq, r+2] <- MSEmin
# Increment counter for next row
qq <- qq + 1
}
}
}
# Compute mean, minimum, and maximum
(MSPR.mean <- cbind(MSPR[,1:2], apply(X=MSPR[,-c(1,2)], MARGIN=1, FUN=mean)))
(MSPR.min <- cbind(MSPR[,1:2], apply(X=MSPR[,-c(1,2)], MARGIN=1, FUN=min)))
(MSPR.max <- cbind(MSPR[,1:2], apply(X=MSPR[,-c(1,2)], MARGIN=1, FUN=max)))
(MSE.mean <- cbind(MSE[,1:2], apply(X=MSE[,-c(1,2)], MARGIN=1, FUN=mean)))
(MSE.min <- cbind(MSE[,1:2], apply(X=MSE[,-c(1,2)], MARGIN=1, FUN=min)))
(MSE.max <- cbind(MSE[,1:2], apply(X=MSE[,-c(1,2)], MARGIN=1, FUN=max)))
# Plot results.
siz.dec <- paste(MSPR[,1],MSPR[,2])
x11(pointsize=6)
boxplot.matrix(x=sqrt(MSPR[,-c(1,2)]), use.cols=FALSE, names=siz.dec)
set.seed(41891019)
reps=10 # Boot Reps, Feel free to increase this to tolerable value
nets=20 # Number of nnets per setting.  May or may not be enough!
siz <- c(1,2,4,6)
dec <- c(0,0.001,0.1,1)
siz <- c(1,2,4,6)
dec <- c(0,0.001,0.1,1)
# Set up matrices to hold results. First two columns are parameter values.
#  Each column after that is a rep.
MSPR <- matrix(NA, nrow=length(siz)*length(dec), ncol=reps+2)
MSE <- matrix(NA, nrow=length(siz)*length(dec), ncol=reps+2)
r=1
# Start data resampling loop
for(r in 1:reps){
resamp <- sample.int(n=nrow(aq), size=nrow(aq), replace=TRUE)
x.b1 <- aq[resamp,2:3]
xr <- rescale(x.b1,x.b1)
yr <- aq[resamp,1]
x.b2 <- aq[-unique(resamp),2:3]
xp <- rescale(x.b2, x.b1)
yp <- aq[-unique(resamp),1]
# Set counter for storage of results
qq <- 1
# Cycle over all parameter values
for(s in siz){
for(d in dec){
MSPR[qq,1:2] <- c(s,d)
MSE[qq,1:2] <- c(s,d)
# Run nnet and get MSPE and MSE from run
MSEmin <- 9e99
for(i in 1:nets){
nn <- nnet(y=yr, x=xr, linout=TRUE, size=s, decay=d, maxit=500, trace=FALSE)
MS <- nn$value/nrow(xr)
if(MS < MSEmin){
MSEmin <- MS
p.min <-predict(nn, newdata=xp)
}
}
# Save results in new column of matrix
MSPR[qq, r+2] <- mean((yp - p.min)^2)
MSE[qq, r+2] <- MSEmin
# Increment counter for next row
qq <- qq + 1
}
}
}
MSPR
y.1
set.seed(41891019)
reps=10 # Boot Reps, Feel free to increase this to tolerable value
nets=20 # Number of nnets per setting.  May or may not be enough!
siz <- c(1,2,4,6)
dec <- c(0,0.001,0.1,1)
# Set up matrices to hold results. First two columns are parameter values.
#  Each column after that is a rep.
MSPR <- matrix(NA, nrow=length(siz)*length(dec), ncol=reps+2)
MSE <- matrix(NA, nrow=length(siz)*length(dec), ncol=reps+2)
r=1
# Start data resampling loop
for(r in 1:reps){
resamp <- sample.int(n=nrow(aq), size=nrow(aq), replace=TRUE)
x.b1 <- aq[resamp,2:3]
xr <- rescale(x.b1,x.b1)
yr <- aq[resamp,1]
x.b2 <- aq[-unique(resamp),2:3]
xp <- rescale(x.b2, x.b1)
yp <- aq[-unique(resamp),1]
# Set counter for storage of results
qq <- 1
# Cycle over all parameter values
for(s in siz){
for(d in dec){
MSPR[qq,1:2] <- c(s,d)
MSE[qq,1:2] <- c(s,d)
# Run nnet and get MSPE and MSE from run
MSEmin <- 9e99
for(i in 1:nets){
nn <- nnet(y=yr, x=xr, linout=TRUE, size=s, decay=d, maxit=500, trace=FALSE)
MS <- nn$value/nrow(xr)
if(MS < MSEmin){
MSEmin <- MS
p.min <-predict(nn, newdata=xp)
}
}
# Save results in new column of matrix
MSPR[qq, r+2] <- mean((yp - p.min)^2)
MSE[qq, r+2] <- MSEmin
# Increment counter for next row
qq <- qq + 1
}
}
}
# Compute mean, minimum, and maximum
(MSPR.mean <- cbind(MSPR[,1:2], apply(X=MSPR[,-c(1,2)], MARGIN=1, FUN=mean)))
(MSPR.min <- cbind(MSPR[,1:2], apply(X=MSPR[,-c(1,2)], MARGIN=1, FUN=min)))
(MSPR.max <- cbind(MSPR[,1:2], apply(X=MSPR[,-c(1,2)], MARGIN=1, FUN=max)))
(MSE.mean <- cbind(MSE[,1:2], apply(X=MSE[,-c(1,2)], MARGIN=1, FUN=mean)))
(MSE.min <- cbind(MSE[,1:2], apply(X=MSE[,-c(1,2)], MARGIN=1, FUN=min)))
(MSE.max <- cbind(MSE[,1:2], apply(X=MSE[,-c(1,2)], MARGIN=1, FUN=max)))
# Plot results.
siz.dec <- paste(MSPR[,1],MSPR[,2])
x11(pointsize=6)
boxplot.matrix(x=sqrt(MSPR[,-c(1,2)]), use.cols=FALSE, names=siz.dec)
# The first plot has some extreme results in it, and we really only care about the lower ones,
#    so eliminate extremes.
x11(pointsize=6)
boxplot.matrix(x=sqrt(MSPR[which(MSPR.max[,3]<1000),-c(1,2)]), use.cols=FALSE, names=siz.dec[which(MSPR.max[,3]<1000)])
# Make plot relative to best
best <- apply(X=MSPR[,-c(1,2)], MARGIN=2, FUN=min)
x11(pointsize=6)
boxplot.matrix(x=sqrt(t(t(MSPR[which(MSPR.max[,3]<1000),-c(1:2)])/best)), use.cols=FALSE, names=siz.dec[which(MSPR.max[,3]<1000)])
# Best results are at boundary, so try some more values
siz <- c(4,5,6,8)
dec <- c(1,1.5,2)
# Set up matrices to hold results. First two columns are parameter values.
#  Each column after that is a rep.
MSPR2 <- matrix(NA, nrow=length(siz)*length(dec), ncol=reps+2)
MSE2 <- matrix(NA, nrow=length(siz)*length(dec), ncol=reps+2)
boxplot.matrix(x=sqrt(t(t(MSPR[which(MSPR.max[,3]<1000),-c(1:2)])/best)), use.cols=FALSE, names=siz.dec[which(MSPR.max[,3]<1000)])
# Best results are at boundary, so try some more values
siz <- c(4,5,6,8)
dec <- c(1,1.5,2)
# Set up matrices to hold results. First two columns are parameter values.
#  Each column after that is a rep.
MSPR2 <- matrix(NA, nrow=length(siz)*length(dec), ncol=reps+2)
MSE2 <- matrix(NA, nrow=length(siz)*length(dec), ncol=reps+2)
# Start data resampling loop
for(r in 1:reps){
resamp <- sample.int(n=nrow(aq), size=nrow(aq), replace=TRUE)
x.b1 <- aq[resamp,2:3]
xr <- rescale(x.b1,x.b2)
yr <- aq[resamp,1]
x.b2 <- aq[-unique(resamp),2:3]
xp <- rescale(x.b2, x.b1)
yp <- aq[-unique(resamp),1]
# Set counter for storage of results
qq <- 1
# Cycle over all parameter values
for(s in siz){
for(d in dec){
MSPR2[qq,1:2] <- c(s,d)
MSE2[qq,1:2] <- c(s,d)
# Run nnet and get MSPE and MSE from run
MSEmin <- 9e99
for(i in 1:50){
nn <- nnet(y=yr, x=xr, linout=TRUE, size=s, decay=d, maxit=500, trace=FALSE)
MS <- nn$value/nrow(xr)
if(MS < MSEmin){
MSEmin <- MS
p.min <-predict(nn, newdata=xp)
}
}
# Save results in new column of matrix
MSPR2[qq, r+2] <- mean((yp - p.min)^2)
MSE2[qq, r+2] <- MSEmin
# Increment counter for next row
qq <- qq + 1
}
}
}
# Compute mean, minimum, and maximum
(MSPR2.mean <- cbind(MSPR2[,1:2], apply(X=MSPR2[,-c(1,2)], MARGIN=1, FUN=mean)))
(MSPR2.min <- cbind(MSPR2[,1:2], apply(X=MSPR2[,-c(1,2)], MARGIN=1, FUN=min)))
(MSPR2.max <- cbind(MSPR2[,1:2], apply(X=MSPR2[,-c(1,2)], MARGIN=1, FUN=max)))
(MSE2.mean <- cbind(MSE2[,1:2], apply(X=MSE2[,-c(1,2)], MARGIN=1, FUN=mean)))
(MSE2.min <- cbind(MSE2[,1:2], apply(X=MSE2[,-c(1,2)], MARGIN=1, FUN=min)))
(MSE2.max <- cbind(MSE2[,1:2], apply(X=MSE2[,-c(1,2)], MARGIN=1, FUN=max)))
# Plot results.
siz.dec <- paste(MSPR2[,1],MSPR2[,2])
x11(pointsize=6)
boxplot.matrix(x=sqrt(MSPR2[,-c(1,2)]), use.cols=FALSE, names=siz.dec)
# Make plot relative to best
best <- apply(X=MSPR2[,-c(1,2)], MARGIN=2, FUN=min)
x11(pointsize=6)
boxplot.matrix(x=sqrt(t(t(MSPR2[,-c(1:2)])/best)), use.cols=FALSE, names=siz.dec)
#########
# Plot results.
siz.dec <- paste(MSPR2[,1],MSPR2[,2])
x11(pointsize=6)
boxplot.matrix(x=sqrt(MSPR2[,-c(1,2)]), use.cols=FALSE, names=siz.dec)
# The first plot has some extreme results in it, and we really only care about the lower ones,
#    so eliminate extremes.
x11(pointsize=6)
boxplot.matrix(x=sqrt(MSPR2[which(MSPR2.max[,3]<1000),-c(1,2)]), use.cols=FALSE, names=siz.dec[which(MSPR2.max[,3]<1000)])
# Make plot relative to best
best <- apply(X=MSPR2[,-c(1,2)], MARGIN=2, FUN=min)
x11(pointsize=6)
boxplot.matrix(x=sqrt(t(t(MSPR2[,-c(1:2)])/best)), use.cols=FALSE, names=siz.dec)
x.1.unscaled <- aq[, 2:3]
y.1 <- aq[,1]
x.1 <- rescale(x.1.unscaled, x.1.unscaled)
nn.best <- run.nn(4,2)
x1 <- seq(from=0, to=22, by=1)
xy1 <- data.frame(expand.grid(Wind=seq(from=0, to=22, by=1),
Temp=seq(from=55, to=98, by=1)))
pred.full <- predict(nn.best,newdata=rescale(xy1,x.1.unscaled))
surface.full = matrix(predict(nn.best, newdata=rescale(xy1,x.1.unscaled)),
nrow=length(x1))
library(rgl)
surface.full
plot(surface.full)
surface.full %>% head()
library(rgl)
open3d()
open3d()
persp3d(x = seq(from=0, to=22, by=1), y = seq(from=55, to=98, by=1),
z = surface.full ,col = "orange", xlab="Wind", ylab="Temp",
zlab="Predicted Ozone", zlim=c(0,200))
points3d(airquality$Ozone ~ airquality$Wind + airquality$Temp, col="blue")
result.default
result.half
result.double
rm(list=ls())                         # Empty the workspace        #
########################
# Data importing
########################
prostate <-  read.table("C:/Users/JinCheol Choi/Documents/Prostate.csv", header=TRUE, sep=",", na.strings=" ")
head(prostate)
geiger=read.table("geiger.txt",header=T)
#-----Geiger counter data analysis ------
getwd()
