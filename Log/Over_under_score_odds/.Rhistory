"dtes_l6m",
#"admin_narcan_fq_ever",
"daily_noninj_marij_l6m", Res_Var)])
Res_Var
Res_Vars
Res_Var="I_called_911"
#************************************
# Little's MCAR test on original data
#************************************
# Tests the null hypothesis that the missing data is Missing Completely At Random (MCAR)
Little.MCAR.test=LittleMCAR(Cross_Sectional_Restricted_Data_to_use[, .SD, .SDcols=c("age",
"dtes_l6m",
#"admin_narcan_fq_ever",
"daily_noninj_marij_l6m", Res_Var)])
LittleMCAR<-function(x){
mysort<-function(x)
{
# Sorts rows and cols of incoming dataframe x into/
# an order for which it is easier to write the likelihood function
nvars<-ncol(x)
powers<-as.integer(2^((nvars-1):0))
binrep<-ifelse(is.na(x),0,1)
decrep<-binrep %*% powers
sorted<-x[order(decrep),]
decrep<-decrep[order(decrep)]
list(sorted.data=sorted, freq=as.vector(table(decrep)))
}
mlest<-function(data,...)
{
# Takes MVN data with missing values and calculates the MLE of the mean vector and the var-cov matrix
data<-as.matrix(data)
sortlist<-mysort(data) # put data with identical patterns of missingness together
nvars<-ncol(data)
nobs<-nrow(data)
if(nvars>50)
stop("mlest cannot handle more than 50 variables.")
startvals<-getstartvals(data) # find starting values
lf<-getclf(data=sortlist$sorted.data, freq=sortlist$freq)
mle<-nlm(lf,startvals,...)
muhat<-mle$estimate[1:nvars] # extract estimates of mean
del<-make.del(mle$estimate[-(1:nvars)]) # extract estimates of sigmahat
factor<-solve(del,diag(nvars))
sigmahat<-t(factor) %*% factor
list(muhat=muhat, sigmahat=sigmahat, value=mle$minimum, gradient=mle$gradient,
stop.code=mle$code, iterations=mle$iterations)
}
if (!(is.matrix(x) | is.data.frame(x)))
stop("Data should be a matrix or dataframe")
if (is.data.frame(x))
x <- data.matrix(x)
# define variables
n.var<-ncol(x) # number of variables
n<-nrow(x)  #number of respondents
var.names<-colnames(x)
r <- 1 * is.na(x)
nmis <- as.integer(apply(r, 2, sum))  #number of missing data for each variable
mdp <- (r %*% (2^((1:n.var - 1)))) + 1  #missing data patterns
x.mp<-data.frame(cbind(x,mdp))
colnames(x.mp)<-c(var.names,"MisPat")
n.mis.pat<-length(unique(x.mp$MisPat)) # number of missing data patterns
p<-n.mis.pat-1
gmean<-mlest(x)$muhat #ML estimate of grand mean (assumes Normal dist)
gcov<-mlest(x)$sigmahat #ML estimate of grand covariance (assumes Normal dist)
colnames(gcov)<-rownames(gcov)<-colnames(x)
#recode MisPat variable to go from 1 through n.mis.pat
x.mp$MisPat2<-rep(NA,n)
for (i in 1:n.mis.pat){
x.mp$MisPat2[x.mp$MisPat == sort(unique(x.mp$MisPat), partial=(i))[i]]<- i
}
x.mp$MisPat<-x.mp$MisPat2
x.mp<-x.mp[ , -which(names(x.mp) %in% "MisPat2")]
#make list of datasets for each pattern of missing data
datasets<-list()
for (i in 1:n.mis.pat){
datasets[[paste("DataSet",i,sep="")]]<-x.mp[which(x.mp$MisPat==i),1:n.var]
}
#degrees of freedom
kj<-0
for (i in 1:n.mis.pat){
no.na<-as.matrix(1* !is.na(colSums(datasets[[i]])))
kj<-kj+colSums(no.na)
}
df<-kj -n.var
#Little's chi-square
d2<-0
cat("this could take a while")
for (i in 1:n.mis.pat){
mean<-(colMeans(datasets[[i]])-gmean)
mean<-mean[!is.na(mean)]
keep<-1* !is.na(colSums(datasets[[i]]))
keep<-keep[which(keep[1:n.var]!=0)]
cov<-gcov
cov<-cov[which(rownames(cov) %in% names(keep)) , which(colnames(cov) %in% names(keep))]
d2<-as.numeric(d2+(sum(x.mp$MisPat==i)*(t(mean)%*%solve(cov)%*%mean)))
}
#p-value for chi-square
p.value<-1-pchisq(d2,df)
#descriptives of missing data
amount.missing<-matrix(nmis, 1, length(nmis))
percent.missing<-amount.missing/n
amount.missing<-rbind(amount.missing,percent.missing)
colnames(amount.missing)<-var.names
rownames(amount.missing)<-c("Number Missing", "Percent Missing")
list(chi.square=d2, df=df, p.value=p.value, missing.patterns=n.mis.pat, amount.missing=amount.missing, data=datasets)
}
#************************************
# Little's MCAR test on original data
#************************************
# Tests the null hypothesis that the missing data is Missing Completely At Random (MCAR)
Little.MCAR.test=LittleMCAR(Cross_Sectional_Restricted_Data_to_use[, .SD, .SDcols=c("age",
"dtes_l6m",
#"admin_narcan_fq_ever",
"daily_noninj_marij_l6m", Res_Var)])
LittleMCAR<-function(x){
getstartvals<-function(x,eps=1e-03)
{
# Returns starting values for the relative precision matrix delta
n<-ncol(x)
startvals<-double(n+n*(n+1)/2)
startvals[1:n]<-apply(x,2,mean,na.rm=TRUE)
sampmat<-cov(x,use="p") # sample var-cov matrix
eig<-eigen(sampmat,symmetric=TRUE)
realvals<-sapply(eig$values, function(y) ifelse(is.complex(y),0,y))
smalleval<-eps*min(realvals[realvals>0])
posvals<-pmax(smalleval,realvals)
mypdmat<-eig$vectors %*% diag(posvals) %*% t(eig$vectors)
myfact<-chol(mypdmat)
mydel<-solve(myfact,diag(n))
signchange<-diag(ifelse(diag(mydel)>0,1,-1))
mydel<-mydel %*% signchange # ensure that diagonal elts are positive
startvals[(n+1):(2*n)]<-log(diag(mydel))
for(i in 2:n){   # assume n>2
startvals[(2*n+sum(1:(i-1))-i+2):(2*n+sum(1:(i-1)))]<-mydel[1:(i-1),i]
}
startvals
}
mysort<-function(x)
{
# Sorts rows and cols of incoming dataframe x into/
# an order for which it is easier to write the likelihood function
nvars<-ncol(x)
powers<-as.integer(2^((nvars-1):0))
binrep<-ifelse(is.na(x),0,1)
decrep<-binrep %*% powers
sorted<-x[order(decrep),]
decrep<-decrep[order(decrep)]
list(sorted.data=sorted, freq=as.vector(table(decrep)))
}
mlest<-function(data,...)
{
# Takes MVN data with missing values and calculates the MLE of the mean vector and the var-cov matrix
data<-as.matrix(data)
sortlist<-mysort(data) # put data with identical patterns of missingness together
nvars<-ncol(data)
nobs<-nrow(data)
if(nvars>50)
stop("mlest cannot handle more than 50 variables.")
startvals<-getstartvals(data) # find starting values
lf<-getclf(data=sortlist$sorted.data, freq=sortlist$freq)
mle<-nlm(lf,startvals,...)
muhat<-mle$estimate[1:nvars] # extract estimates of mean
del<-make.del(mle$estimate[-(1:nvars)]) # extract estimates of sigmahat
factor<-solve(del,diag(nvars))
sigmahat<-t(factor) %*% factor
list(muhat=muhat, sigmahat=sigmahat, value=mle$minimum, gradient=mle$gradient,
stop.code=mle$code, iterations=mle$iterations)
}
if (!(is.matrix(x) | is.data.frame(x)))
stop("Data should be a matrix or dataframe")
if (is.data.frame(x))
x <- data.matrix(x)
# define variables
n.var<-ncol(x) # number of variables
n<-nrow(x)  #number of respondents
var.names<-colnames(x)
r <- 1 * is.na(x)
nmis <- as.integer(apply(r, 2, sum))  #number of missing data for each variable
mdp <- (r %*% (2^((1:n.var - 1)))) + 1  #missing data patterns
x.mp<-data.frame(cbind(x,mdp))
colnames(x.mp)<-c(var.names,"MisPat")
n.mis.pat<-length(unique(x.mp$MisPat)) # number of missing data patterns
p<-n.mis.pat-1
gmean<-mlest(x)$muhat #ML estimate of grand mean (assumes Normal dist)
gcov<-mlest(x)$sigmahat #ML estimate of grand covariance (assumes Normal dist)
colnames(gcov)<-rownames(gcov)<-colnames(x)
#recode MisPat variable to go from 1 through n.mis.pat
x.mp$MisPat2<-rep(NA,n)
for (i in 1:n.mis.pat){
x.mp$MisPat2[x.mp$MisPat == sort(unique(x.mp$MisPat), partial=(i))[i]]<- i
}
x.mp$MisPat<-x.mp$MisPat2
x.mp<-x.mp[ , -which(names(x.mp) %in% "MisPat2")]
#make list of datasets for each pattern of missing data
datasets<-list()
for (i in 1:n.mis.pat){
datasets[[paste("DataSet",i,sep="")]]<-x.mp[which(x.mp$MisPat==i),1:n.var]
}
#degrees of freedom
kj<-0
for (i in 1:n.mis.pat){
no.na<-as.matrix(1* !is.na(colSums(datasets[[i]])))
kj<-kj+colSums(no.na)
}
df<-kj -n.var
#Little's chi-square
d2<-0
cat("this could take a while")
for (i in 1:n.mis.pat){
mean<-(colMeans(datasets[[i]])-gmean)
mean<-mean[!is.na(mean)]
keep<-1* !is.na(colSums(datasets[[i]]))
keep<-keep[which(keep[1:n.var]!=0)]
cov<-gcov
cov<-cov[which(rownames(cov) %in% names(keep)) , which(colnames(cov) %in% names(keep))]
d2<-as.numeric(d2+(sum(x.mp$MisPat==i)*(t(mean)%*%solve(cov)%*%mean)))
}
#p-value for chi-square
p.value<-1-pchisq(d2,df)
#descriptives of missing data
amount.missing<-matrix(nmis, 1, length(nmis))
percent.missing<-amount.missing/n
amount.missing<-rbind(amount.missing,percent.missing)
colnames(amount.missing)<-var.names
rownames(amount.missing)<-c("Number Missing", "Percent Missing")
list(chi.square=d2, df=df, p.value=p.value, missing.patterns=n.mis.pat, amount.missing=amount.missing, data=datasets)
}
#************************************
# Little's MCAR test on original data
#************************************
# Tests the null hypothesis that the missing data is Missing Completely At Random (MCAR)
Little.MCAR.test=LittleMCAR(Cross_Sectional_Restricted_Data_to_use[, .SD, .SDcols=c("age",
"dtes_l6m",
#"admin_narcan_fq_ever",
"daily_noninj_marij_l6m", Res_Var)])
LittleMCAR<-function(x){
getclf<-function(data, freq)
{
# Takes a sorted data frame and returns a likelihood function to be minimized
nvars<-ncol(data)
pars<-double(nvars+nvars*(nvars+1)/2)
testdata<-data[cumsum(freq),]
presabs<-ifelse(is.na(testdata),0,1)
data<-t(data)   # convert data to vectors that can be passed to C
presabs<-t(presabs)
dim(presabs)<-NULL
dim(data)<-NULL
data<-data[!is.na(data)]
#    if (!is.loaded(symbol.C("evallf"))) {
#        cat("loading object code...\n")
#        dyn.load("st771/libs/st771.so")
#    }
function(pars){
.C("evallf",as.double(data),as.integer(nvars),as.integer(freq),
as.integer(x=length(freq)),as.integer(presabs),as.double(pars),val=double(1),PACKAGE="mvnmle")$val;
}
}
getstartvals<-function(x,eps=1e-03)
{
# Returns starting values for the relative precision matrix delta
n<-ncol(x)
startvals<-double(n+n*(n+1)/2)
startvals[1:n]<-apply(x,2,mean,na.rm=TRUE)
sampmat<-cov(x,use="p") # sample var-cov matrix
eig<-eigen(sampmat,symmetric=TRUE)
realvals<-sapply(eig$values, function(y) ifelse(is.complex(y),0,y))
smalleval<-eps*min(realvals[realvals>0])
posvals<-pmax(smalleval,realvals)
mypdmat<-eig$vectors %*% diag(posvals) %*% t(eig$vectors)
myfact<-chol(mypdmat)
mydel<-solve(myfact,diag(n))
signchange<-diag(ifelse(diag(mydel)>0,1,-1))
mydel<-mydel %*% signchange # ensure that diagonal elts are positive
startvals[(n+1):(2*n)]<-log(diag(mydel))
for(i in 2:n){   # assume n>2
startvals[(2*n+sum(1:(i-1))-i+2):(2*n+sum(1:(i-1)))]<-mydel[1:(i-1),i]
}
startvals
}
mysort<-function(x)
{
# Sorts rows and cols of incoming dataframe x into/
# an order for which it is easier to write the likelihood function
nvars<-ncol(x)
powers<-as.integer(2^((nvars-1):0))
binrep<-ifelse(is.na(x),0,1)
decrep<-binrep %*% powers
sorted<-x[order(decrep),]
decrep<-decrep[order(decrep)]
list(sorted.data=sorted, freq=as.vector(table(decrep)))
}
mlest<-function(data,...)
{
# Takes MVN data with missing values and calculates the MLE of the mean vector and the var-cov matrix
data<-as.matrix(data)
sortlist<-mysort(data) # put data with identical patterns of missingness together
nvars<-ncol(data)
nobs<-nrow(data)
if(nvars>50)
stop("mlest cannot handle more than 50 variables.")
startvals<-getstartvals(data) # find starting values
lf<-getclf(data=sortlist$sorted.data, freq=sortlist$freq)
mle<-nlm(lf,startvals,...)
muhat<-mle$estimate[1:nvars] # extract estimates of mean
del<-make.del(mle$estimate[-(1:nvars)]) # extract estimates of sigmahat
factor<-solve(del,diag(nvars))
sigmahat<-t(factor) %*% factor
list(muhat=muhat, sigmahat=sigmahat, value=mle$minimum, gradient=mle$gradient,
stop.code=mle$code, iterations=mle$iterations)
}
if (!(is.matrix(x) | is.data.frame(x)))
stop("Data should be a matrix or dataframe")
if (is.data.frame(x))
x <- data.matrix(x)
# define variables
n.var<-ncol(x) # number of variables
n<-nrow(x)  #number of respondents
var.names<-colnames(x)
r <- 1 * is.na(x)
nmis <- as.integer(apply(r, 2, sum))  #number of missing data for each variable
mdp <- (r %*% (2^((1:n.var - 1)))) + 1  #missing data patterns
x.mp<-data.frame(cbind(x,mdp))
colnames(x.mp)<-c(var.names,"MisPat")
n.mis.pat<-length(unique(x.mp$MisPat)) # number of missing data patterns
p<-n.mis.pat-1
gmean<-mlest(x)$muhat #ML estimate of grand mean (assumes Normal dist)
gcov<-mlest(x)$sigmahat #ML estimate of grand covariance (assumes Normal dist)
colnames(gcov)<-rownames(gcov)<-colnames(x)
#recode MisPat variable to go from 1 through n.mis.pat
x.mp$MisPat2<-rep(NA,n)
for (i in 1:n.mis.pat){
x.mp$MisPat2[x.mp$MisPat == sort(unique(x.mp$MisPat), partial=(i))[i]]<- i
}
x.mp$MisPat<-x.mp$MisPat2
x.mp<-x.mp[ , -which(names(x.mp) %in% "MisPat2")]
#make list of datasets for each pattern of missing data
datasets<-list()
for (i in 1:n.mis.pat){
datasets[[paste("DataSet",i,sep="")]]<-x.mp[which(x.mp$MisPat==i),1:n.var]
}
#degrees of freedom
kj<-0
for (i in 1:n.mis.pat){
no.na<-as.matrix(1* !is.na(colSums(datasets[[i]])))
kj<-kj+colSums(no.na)
}
df<-kj -n.var
#Little's chi-square
d2<-0
cat("this could take a while")
for (i in 1:n.mis.pat){
mean<-(colMeans(datasets[[i]])-gmean)
mean<-mean[!is.na(mean)]
keep<-1* !is.na(colSums(datasets[[i]]))
keep<-keep[which(keep[1:n.var]!=0)]
cov<-gcov
cov<-cov[which(rownames(cov) %in% names(keep)) , which(colnames(cov) %in% names(keep))]
d2<-as.numeric(d2+(sum(x.mp$MisPat==i)*(t(mean)%*%solve(cov)%*%mean)))
}
#p-value for chi-square
p.value<-1-pchisq(d2,df)
#descriptives of missing data
amount.missing<-matrix(nmis, 1, length(nmis))
percent.missing<-amount.missing/n
amount.missing<-rbind(amount.missing,percent.missing)
colnames(amount.missing)<-var.names
rownames(amount.missing)<-c("Number Missing", "Percent Missing")
list(chi.square=d2, df=df, p.value=p.value, missing.patterns=n.mis.pat, amount.missing=amount.missing, data=datasets)
}
#************************************
# Little's MCAR test on original data
#************************************
# Tests the null hypothesis that the missing data is Missing Completely At Random (MCAR)
Little.MCAR.test=LittleMCAR(Cross_Sectional_Restricted_Data_to_use[, .SD, .SDcols=c("age",
"dtes_l6m",
#"admin_narcan_fq_ever",
"daily_noninj_marij_l6m", Res_Var)])
install.packages("C:/Temp/mypack_1.0.tar.gz", repos=NULL, type="source")
install.packages("C:/Users/JinCheol Choi/Desktop/mvnmle_0.1-11.1.tar.gz", repos=NULL, type="source")
install.packages("C:/Users/JinCheol Choi/Desktop/mvnmle_0.1-11.1.tar.gz", repos=NULL, type="source")
install.packages("Rtools")
library(Rtools)
pars
.C
?.C
readLines
sessionInfo()
sessionInfo()
install.packages("rtools40")
install.packages("C:/Users/JinCheol Choi/Desktop/mvnmle_0.1-11.1.tar.gz", repos=NULL, type="source")
install.packages("C:/Users/JinCheol Choi/Desktop/mvnmle_0.1-11.1.tar.gz", repos=NULL, type="source")
install.packages("C:/Users/JinCheol Choi/Desktop/mvnmle_0.1-11.1.tar.gz", repos=NULL, type="source")
library(rtool40)
Sys.which("make")
writeLines('PATH="${RTOOLS40_HOME}\\usr\\bin;${PATH}"', con = "~/.Renviron")
Sys.which("make")
R.version
rm(list=ls())
#*******************
#
# set directory path
#
#*******************
# CODE.dir.1="C:/Users/JinCheol Choi/Desktop/R/Functions/"
# CODE.dir.2="C:/Users/JinCheol Choi/Desktop/R/Soccer_Analysis/"
# CODE.dir.1="C:/Users/JinCheol Choi/Desktop/R/Functions/"
# CODE.dir.2="C:/Users/JinCheol Choi/Desktop/R/Soccer_Analysis/"
CODE.dir.1="C:/Users/JinCheol Choi/Desktop/R/Functions/"
CODE.dir.2="C:/Users/JinCheol Choi/Desktop/R/Soccer_Analysis/"
Year=2020
Countries=c(
"england",
"spain",
"italy",
"netherlands",
"germany",
"china",
"japan",
"turkey"
)
# england : "premier-league", "championship", "league-one", "league-two"
# spain : "laliga"
# italy : "serie-a"
# netherlands : "eerste-divisie"
# germany : "3-liga"
# china : "super-league"
# japan : "j1-league", "j2-league"
# turkey : "super-lig
Leagues=c(
"premier-league",
"championship",
"league-one",
"league-two",
"laliga",
"serie-a",
"eerste-divisie",
"3-liga",
"super-league",
"j1-league",
"j2-league",
"super-lig"
)
source(paste0(CODE.dir.1, "Functions.R"))
source(paste0(CODE.dir.2, "SA_Functions.R"))
lapply(c("data.table",
"rvest",
"stringr",
"dplyr",
"magrittr",
"RSelenium",
"profvis",
"ggplot2",
"readr", # readr::parse_number
"reshape2"), # melt() and dcast()
checkpackages)
#*******************************
#
# Extract_Data_Game_Results ----
#
#*******************************
Years=2020
data.dir.1="C:/Users/JinCheol Choi/Desktop/R/Soccer_Analysis/Data/Game_results/"
lapply(c("data.table",
"rvest"), checkpackages)
source(paste0(CODE.dir.2, "Extract_Data_Game_Results.R"))
#
#********************************
# load
#*****
#load("C:/Users/JinCheol Choi/Desktop/R/Soccer_Analysis/Rdata/2020-10-24-Uni-Dist.Rdata")
load("C:/Users/JinCheol Choi/Desktop/R/Soccer_Analysis/Rdata/2020-10-11-to-25.Rdata")
Optimal_Settings
Optimal_Pars="No" # use optimal parameters based on Optimal_Settings
Prob_Estimate="Exact"
Chosen_Profit_Criteria=1
Coef=1/5.8
data.dir.1="C:/Users/JinCheol Choi/Desktop/R/Soccer_Analysis/Data/Game_results/"
# data.dir.2="C:/Users/JinCheol Choi/Desktop/R/Soccer_Analysis/Data/Over_under_score_odds/"
output.dir="C:/Users/JinCheol Choi/Desktop/R/Soccer_Analysis/Output/Over_under_score_odds/"
log.dir="C:/Users/JinCheol Choi/Desktop/R/Soccer_Analysis/Log/Over_under_score_odds/"
source(paste0(CODE.dir.1, "Functions.R"))
source(paste0(CODE.dir.2, "SA_Functions.R"))
#*********
# save log
#*********
setwd(paste0(log.dir))
#setwd(paste0(log.dir,  Country, "/", League, "/"))
Loop=0
while(Loop==0){
# generate log file
New_Log=0
Log_N=1
while(New_Log==0){
Log_File_Lists=list.files(log.dir)
File_Name=paste0(Sys.Date(), "-", Log_N, ".txt")
if(sum(Log_File_Lists%in%File_Name)>0){
Log_N=Log_N+1
}else{
my_log=file(File_Name)
New_Log=1
}
}
sink(my_log, append = TRUE, type = "output") # Writing console output to log file
sink(my_log, append = TRUE, type = "message")
# In docker, run the following line first
#docker run -d -p 4445:4444 selenium/standalone-chrome:3.141.59
system_sleep=3
source(paste0(CODE.dir.2, "Extract_Over_under_score_odds.R"))
# Close connection to log file
closeAllConnections()
# loop every 6 hours
Sys.sleep(12*60*60)
}
